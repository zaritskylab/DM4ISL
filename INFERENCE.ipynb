{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904abea2-cf4f-4df9-9b59-05a954efec6b",
   "metadata": {},
   "source": [
    "# Inference (after training a model)\n",
    "- Use this notebook to predict flourescence images from brightfield for a certain organelle\n",
    "- you need a trained model and brightfield patches folder\n",
    "- insert the path prefix for the github directory in \"main_path\"\n",
    "- choose the organelle to predict\n",
    "- choose how many patches from the BF folder you want to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9c144a-6c9b-4dab-9bdb-2c06d75f71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/.../\" ## change to main directory of github project\n",
    "organelle = 'NucEnv' # # NucEnv , Nuclioli , DNAmito , ER , AF , Mito , Membrane , Micro , TJ\n",
    "Nimgs = 5 ## how many patches to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d822aa-8b49-489b-ae17-730baf01b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "# from skimage import measure\n",
    "from skimage.feature import hog\n",
    "import sklearn\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from glrlm import GLRLM\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "import requests\n",
    "import cv2 \n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append(main_path)\n",
    "from generative.metrics import FIDMetric, MMDMetric, MultiScaleSSIMMetric, SSIMMetric\n",
    "from generative.networks.nets.diffusion_model_unet import DiffusionModelUNet # Adapted from https://github.com/huggingface/diffusers\n",
    "from generative.networks.schedulers.ddpm import DDPMScheduler\n",
    "from generative.networks.schedulers import DDIMScheduler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "sys.path.append(main_path+'src')\n",
    "from src.ProcessingFunctions import segmentation_pipeline, minmax_norm\n",
    "from src.Params import SegmentationParams\n",
    "from src.DisplayFunctions import display_images, volumetric2sequence\n",
    "from src.LoadSaveFunctions import load_patches, LoadModel, save_patches\n",
    "from src.PredictFL import predict_FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448bba9-73c1-4f63-baa7-e8bbb707caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters and gpu\n",
    "\n",
    "seg_params = SegmentationParams(organelle)\n",
    "device = torch.device(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19bf99-1e90-4078-be6d-b07b10f2fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load BF patches\n",
    "BF_images = load_patches(main_path, organelle, 'BF', Nimgs) # (5, 16, 64, 64) 0-1\n",
    "BF_images = torch.tensor(BF_images, dtype=torch.float32).unsqueeze(1).permute(0,1,3,4,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414b146-fede-4adf-a30d-d790dcc1a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "DiffModel = LoadModel(main_path, organelle, load_model=1, timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177189c-7454-4987-8906-49a17937d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "### run predictions - outputs DDPM, DDPMavg, var and all intermidiate predictions between 2 timesteps\n",
    "\n",
    "t_low, t_high = 200 , 800\n",
    "pred, pred_avg, pred_var, all_x0_pred = predict_FL(DiffModel, BF_images, t_low, t_high, seed=42) # (5, 1, 64, 64, 16) (5, 1, 64, 64, 16) (5, 1, 64, 64, 16) (5, 600, 1, 64, 64, 16)\n",
    "\n",
    "### create std image and seg std image from the same inference process\n",
    "stds = torch.randn_like(BF_images)[0:1]\n",
    "stds_segs = torch.randn_like(BF_images)[0:1]\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    std_ = all_x0_pred[i].std(axis=0)[0] > 0.06\n",
    "    std_th, std_seg_ = segmentation_pipeline(std_ , filter_type=seg_params.filter_type, k1=seg_params.k1, k2=seg_params.k2, k3=seg_params.k3, filter_kernel=seg_params.filter_kernel, sigma=seg_params.sigma, organelle_th=seg_params.organelle_th, do_erode_dilate=seg_params.do_erode_dilate, do_remove_small_objects=seg_params.do_remove_small_objects, do_fill_holes=seg_params.do_fill_holes, do_fill_holes_boarders=seg_params.do_fill_holes_boarders)  \n",
    "    stds      = torch.cat(( stds      , torch.tensor(std_).unsqueeze(0).unsqueeze(0) ) , dim=0 )\n",
    "    stds_segs = torch.cat(( stds_segs , torch.tensor(std_seg_).unsqueeze(0).unsqueeze(0) ) , dim=0 )\n",
    "stds = stds[1:].numpy()\n",
    "stds_segs = stds_segs[1:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3403f1-a6be-4067-8745-b9c0722e7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create std image and seg std image from different seeds\n",
    "\n",
    "t_low, t_high = 200 , 800\n",
    "\n",
    "stds_seeds = torch.randn_like(BF_images)[0:1]\n",
    "stds_segs_seeds = torch.randn_like(BF_images)[0:1]\n",
    "\n",
    "k_pred = []\n",
    "k_pred_avg = []\n",
    "Nseeds = 5\n",
    "for i in range(Nseeds):\n",
    "    pred, pred_avg, pred_var, all_x0_pred = predict_FL(DiffModel, BF_images,t_low, t_high, seed=i) # (5, 1, 64, 64, 16) (5, 1, 64, 64, 16) (5, 1, 64, 64, 16) (5, 600, 1, 64, 64, 16)\n",
    "    k_pred.append(pred)\n",
    "    k_pred_avg.append(pred_avg)\n",
    "k_pred = np.array(k_pred)         # (5seeds, 30images, 1, 64, 64, 16) X0 at t=0\n",
    "k_pred_avg = np.array(k_pred_avg) # (5seeds, 30images, 1, 64, 64, 16) X0avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf2540-631d-4413-bbe0-4a8c52ca49d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred)):\n",
    "    std_ = k_pred_avg[:,i,...].std(axis=0)[0].transpose(2,0,1) > 0.1\n",
    "    std_th, std_seg_ = segmentation_pipeline(std_ , filter_type=seg_params.filter_type, k1=seg_params.k1, k2=seg_params.k2, k3=seg_params.k3, filter_kernel=seg_params.filter_kernel, sigma=seg_params.sigma, organelle_th=seg_params.organelle_th, do_erode_dilate=seg_params.do_erode_dilate, do_remove_small_objects=seg_params.do_remove_small_objects, do_fill_holes=seg_params.do_fill_holes, do_fill_holes_boarders=seg_params.do_fill_holes_boarders)  \n",
    "    stds_seeds = torch.cat(( stds_seeds , torch.tensor(std_).permute(1,2,0).unsqueeze(0).unsqueeze(0) ) , dim=0 )\n",
    "    stds_segs_seeds = torch.cat(( stds_segs_seeds , torch.tensor(std_seg_).permute(1,2,0).unsqueeze(0).unsqueeze(0) ) , dim=0 )\n",
    "stds_seeds = stds_seeds[1:].numpy() # (5, 1, 64, 64, 16)\n",
    "stds_segs_seeds = stds_segs_seeds[1:].numpy() # (5, 1, 64, 64, 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121aa2ed-05b7-4ec1-a79e-0ea9ee4921ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving FL patches\n",
    "\n",
    "save_patches(main_path, organelle, pred, 'FL_pred')\n",
    "save_patches(main_path, organelle, pred_avg, 'FLavg_pred')\n",
    "save_patches(main_path, organelle, stds, 'FLavg_std')\n",
    "save_patches(main_path, organelle, stds_segs, 'FLavg_std_seg')\n",
    "save_patches(main_path, organelle, stds_seeds, 'FLavg_std_seeds')\n",
    "save_patches(main_path, organelle, stds_segs_seeds, 'FLavg_std_seg_seeds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f64c3-864e-41c5-a70a-0ad43dcb0ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISLgit",
   "language": "python",
   "name": "islgit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
